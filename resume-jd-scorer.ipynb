{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95817a18",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-05T14:03:15.984390Z",
     "iopub.status.busy": "2022-03-05T14:03:15.982594Z",
     "iopub.status.idle": "2022-03-05T14:04:01.209296Z",
     "shell.execute_reply": "2022-03-05T14:04:01.208591Z",
     "shell.execute_reply.started": "2022-03-05T13:54:50.371865Z"
    },
    "papermill": {
     "duration": 45.245291,
     "end_time": "2022-03-05T14:04:01.209459",
     "exception": false,
     "start_time": "2022-03-05T14:03:15.964168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\r\n",
      "  Downloading pdfplumber-0.6.0.tar.gz (46 kB)\r\n",
      "     |████████████████████████████████| 46 kB 910 kB/s             \r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: pdfminer.six==20211012 in /opt/conda/lib/python3.7/site-packages (from pdfplumber) (20211012)\r\n",
      "Collecting Pillow>=8.4\r\n",
      "  Downloading Pillow-9.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\r\n",
      "     |████████████████████████████████| 4.3 MB 3.2 MB/s            \r\n",
      "\u001b[?25hRequirement already satisfied: Wand>=0.6.7 in /opt/conda/lib/python3.7/site-packages (from pdfplumber) (0.6.7)\r\n",
      "Requirement already satisfied: cryptography in /opt/conda/lib/python3.7/site-packages (from pdfminer.six==20211012->pdfplumber) (35.0.0)\r\n",
      "Requirement already satisfied: chardet in /opt/conda/lib/python3.7/site-packages (from pdfminer.six==20211012->pdfplumber) (4.0.0)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.7/site-packages (from cryptography->pdfminer.six==20211012->pdfplumber) (1.15.0)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography->pdfminer.six==20211012->pdfplumber) (2.21)\r\n",
      "Building wheels for collected packages: pdfplumber\r\n",
      "  Building wheel for pdfplumber (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pdfplumber: filename=pdfplumber-0.6.0-py3-none-any.whl size=33688 sha256=2b5b6d44ec52e9d57667b3a79546aec95954bd2e3bb1ffd81d514dd21d32351e\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/58/56/fe/2e93d842ffa9ea97746c1ab253d43502ed61c0689361a0224e\r\n",
      "Successfully built pdfplumber\r\n",
      "Installing collected packages: Pillow, pdfplumber\r\n",
      "  Attempting uninstall: Pillow\r\n",
      "    Found existing installation: Pillow 8.2.0\r\n",
      "    Uninstalling Pillow-8.2.0:\r\n",
      "      Successfully uninstalled Pillow-8.2.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "explainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\r\n",
      "beatrix-jupyterlab 3.1.4 requires google-cloud-bigquery-storage, which is not installed.\r\n",
      "easyocr 1.4.1 requires Pillow<8.3.0, but you have pillow 9.0.1 which is incompatible.\u001b[0m\r\n",
      "Successfully installed Pillow-9.0.1 pdfplumber-0.6.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Collecting xlrd\r\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\r\n",
      "     |████████████████████████████████| 96 kB 1.6 MB/s             \r\n",
      "\u001b[?25hInstalling collected packages: xlrd\r\n",
      "Successfully installed xlrd-2.0.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Collecting openpyxl\r\n",
      "  Downloading openpyxl-3.0.9-py2.py3-none-any.whl (242 kB)\r\n",
      "     |████████████████████████████████| 242 kB 1.8 MB/s            \r\n",
      "\u001b[?25hCollecting et-xmlfile\r\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\r\n",
      "Installing collected packages: et-xmlfile, openpyxl\r\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.0.9\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Collecting pyspellchecker\r\n",
      "  Downloading pyspellchecker-0.6.3-py3-none-any.whl (2.7 MB)\r\n",
      "     |████████████████████████████████| 2.7 MB 1.9 MB/s            \r\n",
      "\u001b[?25hInstalling collected packages: pyspellchecker\r\n",
      "Successfully installed pyspellchecker-0.6.3\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "! pip install pdfplumber\n",
    "! pip install xlrd\n",
    "! pip install openpyxl\n",
    "! pip install pyspellchecker\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import docx\n",
    "import pdfplumber\n",
    "import string\n",
    "import os\n",
    "import openpyxl\n",
    "import nltk\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b37517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T14:04:01.286617Z",
     "iopub.status.busy": "2022-03-05T14:04:01.285826Z",
     "iopub.status.idle": "2022-03-05T14:04:01.289751Z",
     "shell.execute_reply": "2022-03-05T14:04:01.289035Z",
     "shell.execute_reply.started": "2022-03-05T13:55:22.718255Z"
    },
    "papermill": {
     "duration": 0.053065,
     "end_time": "2022-03-05T14:04:01.289924",
     "exception": false,
     "start_time": "2022-03-05T14:04:01.236859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_text(path):\n",
    "    \n",
    "    if path.endswith('.docx'):\n",
    "        doc = docx.Document(path)\n",
    "        text = \"\"\n",
    "        fullText = []\n",
    "        for para in doc.paragraphs:\n",
    "            fullText.append(para.text)\n",
    "            text = '\\n'.join(fullText)\n",
    "        \n",
    "        return text\n",
    "                \n",
    "    elif path.endswith('.pdf'):\n",
    "        text = \"\"\n",
    "        with pdfplumber.open(path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                fullText = page.extract_text()\n",
    "                text += '\\n' + fullText\n",
    "                \n",
    "        return text\n",
    "\n",
    "# make everything lowercase to reduce vocabulary\n",
    "\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "#remove punctuations\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "# remove whitespace from text\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "#remove stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# remove stopwords function\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    \n",
    "    text_lower = text_lowercase(text)\n",
    "    text_nonpunc = remove_punctuation(text_lower)\n",
    "    text_nospace = remove_whitespace(text_nonpunc)\n",
    "    text_nospace = text_nospace.replace('\\n',' ')\n",
    "    text_cleaned = remove_stopwords(text_nospace)\n",
    "    text_cleaned = \" \".join(text_cleaned)\n",
    "    text_cleaned = re.compile('<.*?>').sub('', text_cleaned) \n",
    "    text_cleaned = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text_cleaned)  \n",
    "    text_cleaned = re.sub('\\s+', ' ', text_cleaned)  \n",
    "    text_cleaned = re.sub(r'\\[[0-9]*\\]',' ',text_cleaned) \n",
    "    text_cleaned = re.sub(r'[^\\w\\s]', '', str(text_cleaned).lower().strip())\n",
    "    text_cleaned = re.sub(r'\\d',' ',text_cleaned) \n",
    "    text_cleaned = re.sub(r'\\s+',' ',text_cleaned)\n",
    "    \n",
    "    return text_cleaned\n",
    "\n",
    "def lemmatizer(keyword):\n",
    "    finallist = []\n",
    "    for keywords in keyword:\n",
    "        finallist.append(wnl.lemmatize(keywords))\n",
    "        \n",
    "    return finallist\n",
    "\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0145c962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T14:04:01.348512Z",
     "iopub.status.busy": "2022-03-05T14:04:01.347417Z",
     "iopub.status.idle": "2022-03-05T14:04:57.932075Z",
     "shell.execute_reply": "2022-03-05T14:04:57.931335Z",
     "shell.execute_reply.started": "2022-03-05T13:55:22.737813Z"
    },
    "papermill": {
     "duration": 56.615,
     "end_time": "2022-03-05T14:04:57.932263",
     "exception": false,
     "start_time": "2022-03-05T14:04:01.317263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reading files \n",
    "\n",
    "jd_pathlist = []\n",
    "resume_pathlist = []\n",
    "\n",
    "for dirname, _, filenames in os.walk('../input/resumejd-scorer'):\n",
    "    for filename in filenames:\n",
    "        path = os.path.join(dirname, filename)\n",
    "        \n",
    "        if \"Job Descriptions\" in path:\n",
    "            \n",
    "            jd_pathlist.append(path)\n",
    "            \n",
    "        elif \"Resumes\" in path:\n",
    "            \n",
    "            resume_pathlist.append(path)\n",
    "            \n",
    "        elif filename.endswith('.xlsx'):\n",
    "            data = pd.read_excel(path, sheet_name = None)\n",
    "            keyword_data = data['Keyword']\n",
    "            master_keyword_list = keyword_data['Keyword'].tolist()\n",
    "            designation_data = data['Designation']\n",
    "            developer_data = data['Developer']\n",
    "            developer_data = developer_data.fillna(\"\")\n",
    "            continue;\n",
    "            \n",
    "\n",
    "path = \"\"\n",
    "var = \"\"\n",
    "\n",
    "for path in jd_pathlist:\n",
    "    \n",
    "    #extracting text from pdf/docx files\n",
    "    jd_text = extract_text(path)\n",
    "    \n",
    "    #word count for jd\n",
    "    wordCount_JD = len(jd_text.split())\n",
    "    \n",
    "    for var in resume_pathlist:\n",
    "        \n",
    "        #extracting text from pdf/docx files\n",
    "        resume_text = extract_text(var)\n",
    "            \n",
    "        #regex to find whether text contains email address\n",
    "        pattern = '\\w[\\w\\.-]*@\\w[\\w\\.-]+\\.\\w+'\n",
    "        emails = re.findall(pattern, resume_text)\n",
    "        \n",
    "        if len(emails)>0:\n",
    "            bonus_email='True'\n",
    "            \n",
    "        else:\n",
    "            bonus_email='False'\n",
    "        \n",
    "        #regex to find whether text contains mobile number\n",
    "        pattern = \"(?:\\+ *)?\\d[\\d\\- ]{10,}\\d\"\n",
    "        mobile = re.findall(pattern, resume_text)\n",
    "        \n",
    "        if len(mobile)>0:\n",
    "            bonus_mobile='True'\n",
    "                        \n",
    "        else:\n",
    "            bonus_mobile='False'\n",
    "        \n",
    "        #regex to find whether text contains linkedIn address\n",
    "        pattern = \"ttps:\\/\\/www.linkedin.com\\/in\\/\"\n",
    "        linkedIn = re.findall(pattern, resume_text)\n",
    "        \n",
    "        if len(linkedIn)>0:\n",
    "            bonus_linkedIn='True'\n",
    "                                    \n",
    "        else:\n",
    "            bonus_linkedIn='False'\n",
    "        \n",
    "        #word count for resume\n",
    "        wordCount_Resume = len(resume_text.split())\n",
    "        \n",
    "        #performing pre processing on both file data\n",
    "        \n",
    "        jd_cleaned = preprocessing(jd_text)\n",
    "        resume_cleaned = preprocessing(resume_text)\n",
    "        \n",
    "        #matching hard skills, soft skills, certifications\n",
    "        skills = []\n",
    "        \n",
    "        def extractor(var):\n",
    "            for i in range(0,len(developer_data[var])):\n",
    "                if developer_data[var][i] != \"\":\n",
    "                    if resume_text.find(developer_data[var][i]) >= 0:\n",
    "                        skills.append(developer_data[var][i])\n",
    "                        \n",
    "        hard_skills = []\n",
    "        soft_skills = []\n",
    "        certifications = []\n",
    "        \n",
    "        extractor('Soft Skills')\n",
    "        soft_skills = skills\n",
    "        skills = []\n",
    "        \n",
    "        extractor('Hard Skills')\n",
    "        hard_skills = skills\n",
    "        skills = []\n",
    "        \n",
    "        extractor('Certifications')\n",
    "        certifications = skills\n",
    "        skills = []\n",
    "        \n",
    "        #extracting keywords from both text and creating final list\n",
    "        resume_keyword = resume_cleaned.split()\n",
    "        jd_keyword = jd_cleaned.split()\n",
    "        \n",
    "        #lemmatizing words in list\n",
    "        resume_list = lemmatizer(resume_keyword)\n",
    "        jd_list = lemmatizer(jd_keyword)\n",
    "\n",
    "        resume_finalList = pd.unique(resume_list).tolist()\n",
    "        jd_finalList = pd.unique(jd_list).tolist()\n",
    "        \n",
    "        #creation of required list\n",
    "        req_list = []\n",
    "\n",
    "        def comp(list1, list2):\n",
    "            for val in list1:\n",
    "                if val in list2:\n",
    "                    req_list.append(val)\n",
    "                    \n",
    "        #this is the list of requirements in JD\n",
    "        comp(jd_finalList,master_keyword_list)\n",
    "        \n",
    "        #creating final available list in resume for that JD\n",
    "        final_list = []\n",
    "\n",
    "        def comp_2(list1, list2):\n",
    "            for val in list1:\n",
    "                if val in list2:\n",
    "                    final_list.append(val)\n",
    "                    \n",
    "        comp_2(resume_finalList,req_list)\n",
    "        \n",
    "        #calculating score acquired by resume for this JD\n",
    "        \n",
    "        score = (len(final_list)/len(req_list))*100\n",
    "        \n",
    "        df = df.append({\"JD Filepath\" : path, \"Resume Filepath\" : var, 'Score' : score, \"Word Count Resume\" : wordCount_Resume,\n",
    "                        \"Word Count JD\" : wordCount_JD, \"Contains Mobile\" : bonus_mobile, \"Contains Email\" : bonus_email,\n",
    "                        \"Contains LinkedIn\" : bonus_linkedIn, \"Soft Skills\" : soft_skills, \"Hard Skills\" : hard_skills,\n",
    "                       \"Certifications\" : certifications}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d29fac7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T14:04:57.992410Z",
     "iopub.status.busy": "2022-03-05T14:04:57.991664Z",
     "iopub.status.idle": "2022-03-05T14:04:57.999798Z",
     "shell.execute_reply": "2022-03-05T14:04:58.000368Z",
     "shell.execute_reply.started": "2022-03-05T13:56:01.946418Z"
    },
    "papermill": {
     "duration": 0.040872,
     "end_time": "2022-03-05T14:04:58.000651",
     "exception": false,
     "start_time": "2022-03-05T14:04:57.959779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('Output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbfd13e",
   "metadata": {
    "papermill": {
     "duration": 0.028147,
     "end_time": "2022-03-05T14:04:58.055914",
     "exception": false,
     "start_time": "2022-03-05T14:04:58.027767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e92fc88",
   "metadata": {
    "papermill": {
     "duration": 0.02646,
     "end_time": "2022-03-05T14:04:58.109629",
     "exception": false,
     "start_time": "2022-03-05T14:04:58.083169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af570a75",
   "metadata": {
    "papermill": {
     "duration": 0.026515,
     "end_time": "2022-03-05T14:04:58.163109",
     "exception": false,
     "start_time": "2022-03-05T14:04:58.136594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f98b8",
   "metadata": {
    "papermill": {
     "duration": 0.026673,
     "end_time": "2022-03-05T14:04:58.216911",
     "exception": false,
     "start_time": "2022-03-05T14:04:58.190238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ca7d0",
   "metadata": {
    "papermill": {
     "duration": 0.026046,
     "end_time": "2022-03-05T14:04:58.270486",
     "exception": false,
     "start_time": "2022-03-05T14:04:58.244440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6209195",
   "metadata": {
    "papermill": {
     "duration": 0.026231,
     "end_time": "2022-03-05T14:04:58.325422",
     "exception": false,
     "start_time": "2022-03-05T14:04:58.299191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b83d1a",
   "metadata": {
    "papermill": {
     "duration": 0.026134,
     "end_time": "2022-03-05T14:04:58.377944",
     "exception": false,
     "start_time": "2022-03-05T14:04:58.351810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca946c4",
   "metadata": {
    "papermill": {
     "duration": 0.026853,
     "end_time": "2022-03-05T14:04:58.431463",
     "exception": false,
     "start_time": "2022-03-05T14:04:58.404610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a6eac",
   "metadata": {
    "papermill": {
     "duration": 0.026145,
     "end_time": "2022-03-05T14:04:58.483977",
     "exception": false,
     "start_time": "2022-03-05T14:04:58.457832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cebe5f2",
   "metadata": {
    "papermill": {
     "duration": 0.026074,
     "end_time": "2022-03-05T14:04:58.536418",
     "exception": false,
     "start_time": "2022-03-05T14:04:58.510344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9111aca3",
   "metadata": {
    "papermill": {
     "duration": 0.026375,
     "end_time": "2022-03-05T14:04:58.589304",
     "exception": false,
     "start_time": "2022-03-05T14:04:58.562929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe320e",
   "metadata": {
    "papermill": {
     "duration": 0.026072,
     "end_time": "2022-03-05T14:04:58.641857",
     "exception": false,
     "start_time": "2022-03-05T14:04:58.615785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 114.110549,
   "end_time": "2022-03-05T14:04:59.883061",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-05T14:03:05.772512",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
